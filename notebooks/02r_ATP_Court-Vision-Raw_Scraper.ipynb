{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02r. ATP Court Vision Raw Data Scraper\n",
    "\n",
    "Notebook will contain codes and functions for scraping the court vision raw data from the ATP website.\n",
    "Output is archived in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "import sys\n",
    "sys._enablelegacywindowsfsencoding() #Deal with pandas problem with reading file with accents in file path i.e Alexis Sánchez, Victor Lindelöf \n",
    "\n",
    "\n",
    "headers = {'User-Agent': \n",
    "           'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'} \n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import base64\n",
    "import cryptography.hazmat.backends\n",
    "import cryptography.hazmat.primitives.ciphers\n",
    "import cryptography.hazmat.primitives.ciphers.algorithms\n",
    "import cryptography.hazmat.primitives.ciphers.modes\n",
    "import cryptography.hazmat.primitives.padding\n",
    "\n",
    "import sys\n",
    "from time import sleep\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decrypting Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDate(t):\n",
    "    #e = datetime.datetime.now().utcoffset().total_seconds() / 60       # ChatGPT suggestion but not needed\n",
    "    #t = t + datetime.timedelta(minutes=e)                              # ChatGPT suggestion but not needed\n",
    "    \n",
    "    t_tstamp = datetime.datetime.utcfromtimestamp(t/1000)\n",
    "    n = t_tstamp.day\n",
    "    r = int(str(n if n >= 10 else \"0\" + str(n))[::-1])\n",
    "    i = t_tstamp.year\n",
    "    a = int(str(i)[::-1])\n",
    "    o = np.base_repr(int(str(t), base=16), 36).lower() + np.base_repr((i + a) * (n + r), 24).lower()\n",
    "    s = len(o)\n",
    "    if s < 14:\n",
    "        o += \"0\" * (14 - s)\n",
    "    elif s > 14:\n",
    "        o = o[:14]\n",
    "    return \"#\" + o + \"$\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(data):\n",
    "    e = formatDate(data['lastModified'])\n",
    "    n = e.encode()\n",
    "    r = e.upper().encode()\n",
    "    cipher = cryptography.hazmat.primitives.ciphers.Cipher(\n",
    "        cryptography.hazmat.primitives.ciphers.algorithms.AES(n),\n",
    "        cryptography.hazmat.primitives.ciphers.modes.CBC(r),\n",
    "        backend=cryptography.hazmat.backends.default_backend()\n",
    "    )\n",
    "    decryptor = cipher.decryptor()\n",
    "    i = decryptor.update(base64.b64decode(data['response'])) + decryptor.finalize()\n",
    "    unpadder = cryptography.hazmat.primitives.padding.PKCS7(128).unpadder()\n",
    "    #return json.loads(unpadder.update(i) + unpadder.finalize().decode('utf-8'))\n",
    "    return json.loads(i.decode(\"utf-8\").replace(i.decode(\"utf-8\")[-1],\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Match Data and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(\"data/atp-tournament-matches/*Masters*.csv\")[2:6] + glob.glob(\"data/atp-tournament-matches/*Masters*.csv\")[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/atp-tournament-matches\\\\matches_ATP-Masters-1000-Indian-Wells_2022.csv',\n",
       " 'data/atp-tournament-matches\\\\matches_ATP-Masters-1000-Madrid_2022.csv',\n",
       " 'data/atp-tournament-matches\\\\matches_ATP-Masters-1000-Miami_2022.csv',\n",
       " 'data/atp-tournament-matches\\\\matches_ATP-Masters-1000-Monte-Carlo_2022.csv',\n",
       " 'data/atp-tournament-matches\\\\matches_ATP-Masters-1000-Rome_2022.csv']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Failed or no Data!\n",
      "47 Failed or no Data!\n",
      "63 Failed or no Data!\n",
      "64 Failed or no Data!\n",
      "65 Failed or no Data!\n",
      "66 Failed or no Data!\n",
      "67 Failed or no Data!\n",
      "68 Failed or no Data!\n",
      "69 Failed or no Data!\n",
      "70 Failed or no Data!\n",
      "71 Failed or no Data!\n",
      "72 Failed or no Data!\n",
      "73 Failed or no Data!\n",
      "74 Failed or no Data!\n",
      "75 Failed or no Data!\n",
      "76 Failed or no Data!\n",
      "77 Failed or no Data!\n",
      "78 Failed or no Data!\n",
      "79 Failed or no Data!\n",
      "80 Failed or no Data!\n",
      "81 Failed or no Data!\n",
      "82 Failed or no Data!\n",
      "83 Failed or no Data!\n",
      "84 Failed or no Data!\n",
      "85 Failed or no Data!\n",
      "86 Failed or no Data!\n",
      "87 Failed or no Data!\n",
      "88 Failed or no Data!\n",
      "89 Failed or no Data!\n",
      "90 Failed or no Data!\n",
      "91 Failed or no Data!\n",
      "92 Failed or no Data!\n",
      "93 Failed or no Data!\n",
      "94 Failed or no Data!\n",
      "3 Failed or no Data!\n",
      "4 Failed or no Data!\n",
      "5 Failed or no Data!\n",
      "6 Failed or no Data!\n",
      "7 Failed or no Data!\n",
      "8 Failed or no Data!\n",
      "9 Failed or no Data!\n",
      "10 Failed or no Data!\n",
      "11 Failed or no Data!\n",
      "12 Failed or no Data!\n",
      "13 Failed or no Data!\n",
      "14 Failed or no Data!\n",
      "15 Failed or no Data!\n",
      "16 Failed or no Data!\n",
      "17 Failed or no Data!\n",
      "18 Failed or no Data!\n",
      "19 Failed or no Data!\n",
      "20 Failed or no Data!\n",
      "21 Failed or no Data!\n",
      "22 Failed or no Data!\n",
      "23 Failed or no Data!\n",
      "24 Failed or no Data!\n",
      "25 Failed or no Data!\n",
      "26 Failed or no Data!\n",
      "27 Failed or no Data!\n",
      "28 Failed or no Data!\n",
      "29 Failed or no Data!\n",
      "30 Failed or no Data!\n",
      "31 Failed or no Data!\n",
      "32 Failed or no Data!\n",
      "33 Failed or no Data!\n",
      "34 Failed or no Data!\n",
      "35 Failed or no Data!\n",
      "36 Failed or no Data!\n",
      "37 Failed or no Data!\n",
      "38 Failed or no Data!\n",
      "39 Failed or no Data!\n",
      "40 Failed or no Data!\n",
      "41 Failed or no Data!\n",
      "42 Failed or no Data!\n",
      "43 Failed or no Data!\n",
      "44 Failed or no Data!\n",
      "45 Failed or no Data!\n",
      "46 Failed or no Data!\n",
      "47 Failed or no Data!\n",
      "48 Failed or no Data!\n",
      "49 Failed or no Data!\n",
      "50 Failed or no Data!\n",
      "51 Failed or no Data!\n",
      "52 Failed or no Data!\n",
      "53 Failed or no Data!\n",
      "54 Failed or no Data!\n",
      "55 Failed or no Data!\n",
      "56 Failed or no Data!\n",
      "57 Failed or no Data!\n",
      "58 Failed or no Data!\n",
      "59 Failed or no Data!\n",
      "60 Failed or no Data!\n",
      "61 Failed or no Data!\n",
      "62 Failed or no Data!\n",
      "5 Failed or no Data!\n",
      "18 Failed or no Data!\n",
      "33 Failed or no Data!\n",
      "45 Failed or no Data!\n",
      "63 Failed or no Data!\n",
      "64 Failed or no Data!\n",
      "65 Failed or no Data!\n",
      "66 Failed or no Data!\n",
      "67 Failed or no Data!\n",
      "68 Failed or no Data!\n",
      "69 Failed or no Data!\n",
      "70 Failed or no Data!\n",
      "71 Failed or no Data!\n",
      "72 Failed or no Data!\n",
      "73 Failed or no Data!\n",
      "74 Failed or no Data!\n",
      "75 Failed or no Data!\n",
      "76 Failed or no Data!\n",
      "77 Failed or no Data!\n",
      "78 Failed or no Data!\n",
      "79 Failed or no Data!\n",
      "80 Failed or no Data!\n",
      "81 Failed or no Data!\n",
      "82 Failed or no Data!\n",
      "83 Failed or no Data!\n",
      "84 Failed or no Data!\n",
      "85 Failed or no Data!\n",
      "86 Failed or no Data!\n",
      "87 Failed or no Data!\n",
      "88 Failed or no Data!\n",
      "89 Failed or no Data!\n",
      "90 Failed or no Data!\n",
      "91 Failed or no Data!\n",
      "92 Failed or no Data!\n",
      "93 Failed or no Data!\n",
      "94 Failed or no Data!\n",
      "103 Failed or no Data!\n",
      "27 Failed or no Data!\n",
      "31 Failed or no Data!\n",
      "32 Failed or no Data!\n",
      "33 Failed or no Data!\n",
      "34 Failed or no Data!\n",
      "35 Failed or no Data!\n",
      "36 Failed or no Data!\n",
      "37 Failed or no Data!\n",
      "38 Failed or no Data!\n",
      "31 Failed or no Data!\n",
      "32 Failed or no Data!\n",
      "33 Failed or no Data!\n",
      "34 Failed or no Data!\n",
      "35 Failed or no Data!\n",
      "36 Failed or no Data!\n",
      "37 Failed or no Data!\n",
      "61 Failed or no Data!\n"
     ]
    }
   ],
   "source": [
    "for file in file_list:\n",
    "\n",
    "    matches = pd.read_csv(file)\n",
    "\n",
    "    # Subset matches to scrape (exclude qualifying)\n",
    "    matches_scp = matches[~matches.Round.str.contains(\"Qualifying\")]\n",
    "\n",
    "    # Loop through matches and scrape one by one \n",
    "    for i in np.arange(0,len(matches_scp)):\n",
    "\n",
    "        try:\n",
    "            _,_,_,_,_,_,_,year,tourn_id,match_id = matches_scp.URL.iloc[i].split('/')\n",
    "        \n",
    "            match_id = match_id.upper()\n",
    "            #print(match_id)\n",
    "\n",
    "            player1 = matches_scp.Player1.iloc[i]\n",
    "            player2 = matches_scp.Player2.iloc[i]\n",
    "\n",
    "            link = f'https://itp-atp-sls.infosys-platforms.com/static/prod/court-vision/{year}/{tourn_id}/{match_id}/data.json'\n",
    "            # Get request and content from the given link and parse into HTML\n",
    "            pageTree = requests.get(link, headers=headers)\n",
    "            pageSoup = BeautifulSoup(pageTree.content, 'html.parser') \n",
    "\n",
    "            results_json = json.loads(str(pageSoup))\n",
    "\n",
    "            # Decode Data\n",
    "            raw_data = decode(results_json)\n",
    "\n",
    "            # Match the formatting of player1/2 to that in the court-vision raw data's player data \n",
    "            # If player names match their respective indexes in the court-vision raw data, \n",
    "            # then we keep the player name order, otherwise we swap \n",
    "            # \"Truncated Name\" for player 1 (e.g. R. NADAL)\n",
    "            player1_tname = player1.split(\" \")[0][0]+\".\" + \" \" + player1.split(\" \")[1].upper()\n",
    "            player1_cv = raw_data['courtVisionData'][0]['a79']['a83'][0]['a85']\n",
    "\n",
    "            if player1_tname == player1_cv:\n",
    "                player1_cvfile = player1\n",
    "                player2_cvfile = player2\n",
    "            else:\n",
    "                player1_cvfile = player2\n",
    "                player2_cvfile = player1\n",
    "\n",
    "            # Formatting\n",
    "            player1_cvfile = player1_cvfile.replace(\" \",\"-\")\n",
    "            player2_cvfile = player2_cvfile.replace(\" \",\"-\")\n",
    "\n",
    "            # Format the \"Round Name\" to appear on file path\n",
    "            round_n = matches_scp.Round.iloc[i]\n",
    "            if \"Round Of\" in round_n:\n",
    "                round_short = round_n.split(\" \")[0][0] + round_n.split(\" \")[-1]\n",
    "            elif \"Round\" in round_n:\n",
    "                round_short = \"\".join([s[0] for s in round_n.split(\" \")])\n",
    "            elif round_n == \"Quarterfinals\" or round_n == \"Quarter-Finals\":\n",
    "                round_short = \"QF\"\n",
    "            elif round_n == \"Semifinals\" or round_n == \"Semi-Finals\":\n",
    "                round_short = \"SF\"\n",
    "            elif round_n == \"Final\" or round_n == \"Finals\":\n",
    "                round_short = \"F\"\n",
    "\n",
    "            # Output the decoded courtvision data into a json file\n",
    "            with open(f\"data/court-vision/{tourn_id}_{round_short}_{player1_cvfile}-vs-{player2_cvfile}_{year}_{match_id}_court-vision.json\", 'w') as fp:\n",
    "                json.dump(raw_data, fp)\n",
    "\n",
    "            sleeptime = np.random.uniform(3, 20)\n",
    "            sleep(sleeptime)\n",
    "\n",
    "        except:\n",
    "            print(f\"{i} Failed or no Data!\")\n",
    "            pass\n",
    "\n",
    "    sleeptime = np.random.uniform(25, 60)\n",
    "    sleep(sleeptime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "footyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0c9a93429a88daf4bf2a05c7cf40bbe3f1288a690c5bbe3660b2858e5ac0985"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
